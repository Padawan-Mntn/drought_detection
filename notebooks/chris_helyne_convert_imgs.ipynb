{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime \n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from google.cloud import storage\n",
    "import tempfile\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "# sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
    "# from drought_detection.data_handling import parse_visual_rgb, get_img_from_example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Files\n",
    "\n",
    "Connect to Google Bucket and verify TFRecord file entries look OK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dirlist(directory):\n",
    "    '''list files in directory'''\n",
    "    return [os.path.join(directory, file) for file in os.listdir(directory) if 'part-' in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sat_file(image_file, bands_):\n",
    "    '''\n",
    "    This function filters satellite image data by specific spectral bands (RGB in this case).\n",
    "    The function loads a batch of satellite images from a list of files\n",
    "    and parses the satellite image data files for some specific features,\n",
    "    e.g. spectral bands (B2, B3, B4, see official documentation)\n",
    "\n",
    "    Input(s): - list of satellite image files (including path, e.g '/data/train/part-r-00000')\n",
    "    Outputs:  - list of dictionaries of raw satellite data (filtered by spectral band)\n",
    "    '''\n",
    "    \n",
    "    # make tfrecord format list for chosen bands\n",
    "    tfrecord_format = {}\n",
    "    for b in bands_:\n",
    "        tfrecord_format[b] = tf.compat.v1.FixedLenFeature([], tf.string)\n",
    "    tfrecord_format['label'] = tf.compat.v1.FixedLenFeature([], tf.int64)\n",
    "    \n",
    "    # load and parse one sat image\n",
    "    dataset = tf.data.TFRecordDataset(image_file)\n",
    "    iterator = tf.compat.v1.data.make_one_shot_iterator(dataset)\n",
    "    parsed_sat_file = [tf.compat.v1.parse_single_example(data, tfrecord_format) for data in iterator]\n",
    "    \n",
    "    return parsed_sat_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that converts a raw sat image (tensorflow object) to matrix of numbers & label (it also scales bands)\n",
    "def transform_sat_img(parsed_sat_file, bands_=['B4', 'B3', 'B2'], intensify_=True):\n",
    "    '''\n",
    "    This function creates a 3D imgArray in shape 65 x 65 x n_bands (65x65 pixels) for\n",
    "    a single parsed satellite image, while also scaling each spectral band.\n",
    "\n",
    "    Parameters:\n",
    "            parsed_sat_img (dict): a parsed satellite image: Specific Tensorflow format (as dictionary)\n",
    "            bands (list): list of bands to process (order is important!)\n",
    "            intensify (bool): whether to scale or not (affects how bright plotted image looks(?))\n",
    "\n",
    "    Returns:\n",
    "            imgArray (tuple): tuple of processed images in n-Dimensional arrays (depends on number of bands chosen)\n",
    "            label (list): list of corresponding labels (as int32)\n",
    "    '''\n",
    "    # convert to image array of numbers and label\n",
    "    n_bands = len(bands_) # number of of bands determines depth of imgArray\n",
    "    imgArray = np.zeros((65,65,n_bands), 'uint8') # create empty array\n",
    "\n",
    "    # transform, reshape, and intensity-scale image data\n",
    "    for i, band in enumerate(bands_): # order of specified bands is important because that is the order they will be appended\n",
    "        band_data = np.frombuffer(parsed_sat_file[0][band].numpy(), dtype=np.uint8) # transforms raw tensorflow data into 1D array\n",
    "        band_data = band_data.reshape(65, 65) # reshapes data into 65 x 65 pixel matrix\n",
    "        if intensify_:\n",
    "            band_data = band_data/np.max(band_data)*255 # scaling digital numbers so image is slightly brighter\n",
    "        else:\n",
    "            band_data = band_data*255 # scaling digital numbers\n",
    "        imgArray[..., i] = band_data\n",
    "\n",
    "    label = tf.cast(parsed_sat_file[0]['label'], tf.int32).numpy() # gets label for image\n",
    "\n",
    "    return imgArray, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(file='../raw_data/val/part-r-00038', bands=['B4', 'B3', 'B2'], intensify=True):\n",
    "    parsed_sat1 = read_sat_file(file, bands_=bands)\n",
    "    imgArray, label = transform_sat_img(parsed_sat1, bands_=bands, intensify_=intensify)\n",
    "    return imgArray, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imgs_set(directory='../raw_data/train/', n_files = 2, bands=['B4', 'B3', 'B2'], intensify=True):\n",
    "    \n",
    "    filenames = []\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    files = dirlist(directory)\n",
    "    \n",
    "    for n in range(n_files):\n",
    "        imgArray, label = load_img(file=files[n], bands=bands, intensify=intensify)\n",
    "        filenames.append(files[n])\n",
    "        images.append(imgArray)\n",
    "        labels.append(label)\n",
    "        \n",
    "    return filenames, images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create Prefetch dataset structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prefetch_dataset(filenames, images, labels):\n",
    "    '''\n",
    "    This function transforms our data into the correct data structure for modelling.\n",
    "\n",
    "    Parameters:\n",
    "            filenames (list): a list of satellite files\n",
    "            images (tuple): tuple of processed images in n-Dimensional arrays (depends on number of bands chosen)\n",
    "            label (list): list of corresponding labels (as int32)\n",
    "\n",
    "    Returns:\n",
    "            dataset (PrefetchDataset):\n",
    "                <PrefetchDataset shapes: {filename: (), image: (65, 65, 3), label: ()}, \n",
    "                types: {filename: tf.string, image: tf.uint8, label: tf.int64}>\n",
    "    '''    \n",
    "    AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices({'filename': filenames,\n",
    "                                                'image': images, \n",
    "                                                'label': labels})\n",
    "    # dataset = dataset.shuffle(2048) # we shuffle later, not sure we need it here\n",
    "    dataset = dataset.prefetch(AUTO)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, (65, 65, 3), [0, 0, 1, 0, 0, 0, 0, 3, 0, 0])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames, images, labels = load_imgs_set(n_files = 10)\n",
    "len(images), images[0].shape, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: ((), (65, 65, 3), ()), types: (tf.string, tf.uint8, tf.int32)>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((filenames, images, labels))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.array(labels)\n",
    "labels = labels.astype('int64')\n",
    "type(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices({'filename': filenames,\n",
    "                                              'image': images, \n",
    "                                              'label': labels})\n",
    "dataset = dataset.shuffle(2048)\n",
    "dataset = dataset.prefetch(AUTO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: {filename: (), image: (65, 65, 3), label: ()}, types: {filename: tf.string, image: tf.uint8, label: tf.int64}>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 2)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tmplist = images, labels\n",
    "\n",
    "# np.expand_dims(tmplist, axis = 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.data.experimental.make_batched_features_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From windspeed notebook\n",
    "\n",
    "https://github.com/h-fuzzy-logic/python-windspeed/blob/main/Windspeed-Predictions-with-TPU.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://wagon-data-batch913-drought_detection/data/train'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# butcket data file location\n",
    "BUCKET_NAME = 'wagon-data-batch913-drought_detection'\n",
    "BUCKET_TRAIN_DATA_PATH = 'data/train'\n",
    "\n",
    "# # GS bucket location path\n",
    "GS_BUCKET_PATH_LINK = f\"gs://{BUCKET_NAME}/{BUCKET_TRAIN_DATA_PATH}\"\n",
    "\n",
    "# check our gs bucket link\n",
    "GS_BUCKET_PATH_LINK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# works for 1 image for 1 band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_one_whole_rec(data, band='B1'):\n",
    "    tfrecord_format = (\n",
    "            {\n",
    "            band: tf.io.FixedLenFeature([], tf.string),    # 0.43 - 0.45 μm Coastal aerosol\n",
    "            'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "            }\n",
    "        )\n",
    "    data = tf.io.parse_example(data, tfrecord_format)\n",
    "    img = tf.io.decode_raw(data[band], tf.uint8)\n",
    "    #expecting this original image size \n",
    "    img = tf.reshape(img, [65,65,1])\n",
    "    lab = data[\"label\"]\n",
    "    return img, lab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: ((65, 65, 1), ()), types: (tf.uint8, tf.int64)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get data\n",
    "train_images = tf.data.TFRecordDataset(\n",
    "    \"gs://wagon-data-batch913-drought_detection/data/val/part-r-00000\"\n",
    ")\n",
    "ds_train = train_images.map(read_one_whole_rec)\n",
    "\n",
    "ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: ((65, 65, 1), ()), types: (tf.uint8, tf.int64)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Windspeed is 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEEAAABBCAAAAACrnJaZAAAFHklEQVR4nG2XUZLrOg5DDyD1vI3M/vc3E4t4H5Rs5/ZNVaeStkQSJAgy+m+t4JkEa+HEEYUAhuNaKQ2SKkTAISgKBGmSAaUqBJaSCFQMxKz8rzSsq5B+AFGEKCYCyCxJiFSs1BCGBMaskVorVERsM7RQYSkISBmvCaEElZksBlK0pBHFoYgRGgCFIhtFChIWYwJRDVmOlDWiEvrHqUIrGsQioIIqTBKhjErFTCJI+ScMVJf8WbIKsZLIafAQk8RZjgDVlag0A2EgDNRiMbiyMq265og/piBASCht0J1TjSkkRThE+odPPJLLCDGkUtIG+iVIpCiVgDXVxneJxH+qmNGYq5glat8X6ThESUX6CjVPWUmkwlNZ9tC2CcQhSn/pnKZUKELSPOe2JaGhJQV0oCciEBElhmYjIIG18QUkTUVaCwztM0lIIMlBUwXaLmoGEhMFScrS5/K+1BFH7suw0aPcaJgALonIUiXXBamUoARyJIW44mJnRF1eEZkQLCIPZ9Xno9GFCgqUGPsCq6vZ+M9rRs1ASUlVYTPSxYscosolspoLgr6xM+3pfdgDJPvqCkYuRJZgjSK7HTuaLR8gmH0+jrTIkknQyb/cbush5XHO7nHT8YlK9d3sulmIacgK0gnhyUGnZJJISvLJuOtHkANhJXVuuJp5CtgrEkiNQu3GRW3iUpQCK8NsNuTUBCGz1Jlkt6kZbrrsaDd03WyitryFUP3MyoSI0nTnPTfQbJ6OrBze3+1DlhLFG4V+TF1I7a77WSTSqHXf2kRE3TM2ljUVg1ZFPq5bD1Tt0ZUOXmj31o4nlqWJVXUJc/sXrtNZnzaXXb2TShCRBJkpV3losTufVuWTxuf8l9R1ShUwk1jeENqs9GTu+XD++RhKkkwcyovsZs7xvDXriNU7hA2oFBP/fKFjt9W+a7WWCfH96mCDsfDQiSgvOxx6KQTyCr7fCwjlilItg/fDp3fGMZQzdI4RbbHXFEhtcOPtNaAKKBiV6Mv7LoLcRZlZc1J3/51821GE0OFCHnC7bkiszKpoOFXRPqVASkA8P9e28TJwyqoeEOj6GaI+11e+96ayQBqrNoNOktSKLxLPUcs0l94F31lZIK68yNFxWGmEyZT8WdTYpT8wdg3EVoINPW/7kqp1EpAPsofCkuSH3+KpCT34WVGM9+q1abUHYjqGlTs7bzo0BXoJsOawFXFoF3mr8O3wEa2+L8lbccHukeXKbeO+fYbPLp4eq/2xLJj5lIjPSPmzHlEORUSH2uFKqiBlfv4vOy1p5/HbyFGut3uC6MiRV0gNcrKQHpHSi8fvqM7AtBqYvJAH61nYnk5+c1R3RjoJScIoEQNzRr+c/en+RnMK1Z4Uyz/+rK/ez2HD29LJ4ROcKRLZc3yup4FfTs6y9oXkoZu3H49xXSc7bzW9I9DzfctP/6VHnEytvKhy05vbar/rNpA0Ow17VV/zFsFWgXcOu7JvFBtxZZtw1r2Uvgjw3lO+hP5ZS9mDxbHqt5T/YedtYkOx9wG/iadnIv6ydkLot+hOsuNOkzg/n/5y8RXg/ThJb+ZHOU5XfL++R3Af0R7Mm/52kbP4CJ15+M6Kvg1yRnsf89yLyUPE18L4t1RqT90jjZ6j0K5OD8Nfv6t+I6tTZIE1va2+Ed+C/2uI6LQeu/vcLfIKXvezvxOiDd+7acwzt8/j5+Nx9jW8c/Srq+jX76c/0b6dPv+VeJNGrmLsQfNnqOfM96PO9FbkiH8ByxH0mgYCW+gAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=65x65 at 0x19B8470D0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image \n",
    "import plotly.express as px\n",
    "\n",
    "for example in ds_train.take(1):\n",
    "  im, lb = example\n",
    "#   print(im) # array of single band values\n",
    "#   px.imshow(im) # needs 3D RGB to plot\n",
    "  print(\"Windspeed is\", str(lb.numpy())) \n",
    "  display(Image.fromarray(tf.squeeze(im).numpy())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_one_whole_rec(data):\n",
    "  tfrecord_format = (\n",
    "        {\n",
    "            \"x\": tf.io.FixedLenFeature([], tf.string),\n",
    "            \"y\": tf.io.FixedLenFeature([], tf.float32)\n",
    "        }\n",
    "   )\n",
    "  \n",
    "  data = tf.io.parse_example(data, tfrecord_format)\n",
    "  img = tf.io.decode_raw(data[\"x\"], tf.uint8)\n",
    "  #expecting this original imgage size \n",
    "  img = tf.reshape(img, [366,366,1])\n",
    "  lab = data[\"y\"]\n",
    "  return img, lab\n",
    "\n",
    "# Train set and validation set were decided outside of this notebook.  \n",
    "# Data was split by storm instead of a random split to keep a storms' images together.  \n",
    "train_images = tf.data.TFRecordDataset(\n",
    "    \"gs://eye-speed/data/tfrecs/train.tfrecords\"\n",
    ")\n",
    "ds_train = train_images.map(read_one_whole_rec)\n",
    "\n",
    "val_images = tf.data.TFRecordDataset(\n",
    "    \"gs://eye-speed/data/tfrecs/val.tfrecords\"\n",
    ")\n",
    "ds_val = val_images.map(read_one_whole_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionDeniedError",
     "evalue": "Error executing an HTTP request: HTTP response code 403 with body '<?xml version='1.0' encoding='UTF-8'?><Error><Code>AccessDenied</Code><Message>Access denied.</Message><Details>drought-detection-service-acco@drought-detection.iam.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object.</Details></Error>'\n\t when reading gs://eye-speed/data/tfrecs/train.tfrecords [Op:IteratorGetNext]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionDeniedError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/q5/2d8pwqf14mv_nz14d23_2b040000gn/T/ipykernel_1666/1673046621.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mds_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Windspeed is\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    759\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    742\u001b[0m     \u001b[0;31m# to communicate that there is no more data to iterate over.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSYNC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m       ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[1;32m    745\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.8/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2726\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2727\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2728\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2729\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2730\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6895\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6896\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6897\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6898\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.8/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mPermissionDeniedError\u001b[0m: Error executing an HTTP request: HTTP response code 403 with body '<?xml version='1.0' encoding='UTF-8'?><Error><Code>AccessDenied</Code><Message>Access denied.</Message><Details>drought-detection-service-acco@drought-detection.iam.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object.</Details></Error>'\n\t when reading gs://eye-speed/data/tfrecs/train.tfrecords [Op:IteratorGetNext]"
     ]
    }
   ],
   "source": [
    "\n",
    "for example in ds_train.take(1):\n",
    "  im, lb = example\n",
    "  print(\"Windspeed is\", str(lb.numpy())) \n",
    "  display(Image.fromarray(tf.squeeze(im).numpy())) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try for single file, multiple bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_one_sat_multi_bands(data):\n",
    "    tfrecord_format = (\n",
    "            {\n",
    "            'B4': tf.io.FixedLenFeature([], tf.string),    \n",
    "            'B3': tf.io.FixedLenFeature([], tf.string),  \n",
    "            'B2': tf.io.FixedLenFeature([], tf.string),  \n",
    "            'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "            }\n",
    "        )\n",
    "    data = tf.io.parse_example(data, tfrecord_format)\n",
    "    \n",
    "    b4 = tf.io.decode_raw(data['B4'], tf.uint8)\n",
    "    b4 = tf.reshape(b4, [65,65,1])\n",
    "    \n",
    "    b3 = tf.io.decode_raw(data['B3'], tf.uint8)\n",
    "    b3 = tf.reshape(b3, [65,65,1])\n",
    "    \n",
    "    b2 = tf.io.decode_raw(data['B2'], tf.uint8)\n",
    "    b2 = tf.reshape(b2, [65,65,1])\n",
    "    img = []\n",
    "    img = [b4, b3, b2]\n",
    "    \n",
    "    #expecting this original image size \n",
    "    lab = data[\"label\"]\n",
    "    return img, lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: ((3, 65, 65, 1), ()), types: (tf.uint8, tf.int64)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get data\n",
    "train_images = tf.data.TFRecordDataset(\n",
    "    \"gs://wagon-data-batch913-drought_detection/data/val/part-r-00000\"\n",
    ")\n",
    "\n",
    "ds_train = train_images.map(read_one_sat_multi_bands)\n",
    "\n",
    "ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: ((3, 65, 65, 1), ()), types: (tf.uint8, tf.int64)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<MapDataset shapes: ((3, 65, 65, 1), ()), types: (tf.uint8, tf.int64)>,\n",
       " <MapDataset shapes: ((3, 65, 65, 1), ()), types: (tf.uint8, tf.int64)>]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trying to loop through multiple files\n",
    "file_list = [\"gs://wagon-data-batch913-drought_detection/data/val/part-r-00000\", \n",
    "             \"gs://wagon-data-batch913-drought_detection/data/val/part-r-00001\"]\n",
    "\n",
    "ds_train = []\n",
    "for f in file_list:\n",
    "    train_images = tf.data.TFRecordDataset(f)\n",
    "    ds_train.append(train_images.map(read_one_sat_multi_bands))\n",
    "    # ds_train.append(img.take(1))\n",
    "\n",
    "# list of MapDataset still not the right object type (also not the right shape)\n",
    "ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: ((3, 65, 65, 1), ()), types: (tf.uint8, tf.int64)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# still not the right object type\n",
    "ds_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[53]\n",
      "   [46]\n",
      "   [31]\n",
      "   ...\n",
      "   [26]\n",
      "   [27]\n",
      "   [33]]\n",
      "\n",
      "  [[50]\n",
      "   [40]\n",
      "   [42]\n",
      "   ...\n",
      "   [43]\n",
      "   [40]\n",
      "   [36]]\n",
      "\n",
      "  [[44]\n",
      "   [44]\n",
      "   [52]\n",
      "   ...\n",
      "   [51]\n",
      "   [57]\n",
      "   [49]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[65]\n",
      "   [62]\n",
      "   [43]\n",
      "   ...\n",
      "   [53]\n",
      "   [49]\n",
      "   [48]]\n",
      "\n",
      "  [[65]\n",
      "   [59]\n",
      "   [41]\n",
      "   ...\n",
      "   [55]\n",
      "   [50]\n",
      "   [52]]\n",
      "\n",
      "  [[59]\n",
      "   [53]\n",
      "   [40]\n",
      "   ...\n",
      "   [52]\n",
      "   [55]\n",
      "   [52]]]\n",
      "\n",
      "\n",
      " [[[39]\n",
      "   [35]\n",
      "   [30]\n",
      "   ...\n",
      "   [25]\n",
      "   [25]\n",
      "   [29]]\n",
      "\n",
      "  [[39]\n",
      "   [33]\n",
      "   [35]\n",
      "   ...\n",
      "   [33]\n",
      "   [32]\n",
      "   [30]]\n",
      "\n",
      "  [[35]\n",
      "   [35]\n",
      "   [39]\n",
      "   ...\n",
      "   [41]\n",
      "   [45]\n",
      "   [39]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[47]\n",
      "   [44]\n",
      "   [33]\n",
      "   ...\n",
      "   [36]\n",
      "   [34]\n",
      "   [32]]\n",
      "\n",
      "  [[48]\n",
      "   [42]\n",
      "   [32]\n",
      "   ...\n",
      "   [37]\n",
      "   [34]\n",
      "   [34]]\n",
      "\n",
      "  [[45]\n",
      "   [39]\n",
      "   [32]\n",
      "   ...\n",
      "   [35]\n",
      "   [36]\n",
      "   [34]]]\n",
      "\n",
      "\n",
      " [[[35]\n",
      "   [32]\n",
      "   [26]\n",
      "   ...\n",
      "   [25]\n",
      "   [26]\n",
      "   [28]]\n",
      "\n",
      "  [[34]\n",
      "   [31]\n",
      "   [32]\n",
      "   ...\n",
      "   [31]\n",
      "   [30]\n",
      "   [28]]\n",
      "\n",
      "  [[33]\n",
      "   [32]\n",
      "   [33]\n",
      "   ...\n",
      "   [36]\n",
      "   [38]\n",
      "   [34]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[39]\n",
      "   [37]\n",
      "   [30]\n",
      "   ...\n",
      "   [31]\n",
      "   [30]\n",
      "   [29]]\n",
      "\n",
      "  [[40]\n",
      "   [35]\n",
      "   [29]\n",
      "   ...\n",
      "   [31]\n",
      "   [30]\n",
      "   [29]]\n",
      "\n",
      "  [[37]\n",
      "   [33]\n",
      "   [29]\n",
      "   ...\n",
      "   [30]\n",
      "   [30]\n",
      "   [29]]]], shape=(3, 65, 65, 1), dtype=uint8)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "px.imshow only accepts 2D single-channel, RGB or RGBA images. An image of shape (3, 65, 65, 1) was provided. Alternatively, 3- or 4-D single or multichannel datasets can be visualized using the `facet_col` or/and `animation_frame` arguments.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/q5/2d8pwqf14mv_nz14d23_2b040000gn/T/ipykernel_1666/1703062884.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#   cast_image = tf.cast(im.swapaxes(0, 3), 'float32')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# array of single band values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# needs 3D RGB to plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m#   print(\"Windspeed is\", str(lb.numpy()))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#   display(Image.fromarray(tf.squeeze(im).numpy()))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.8/site-packages/plotly/express/_imshow.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(img, zmin, zmax, origin, labels, x, y, animation_frame, facet_col, facet_col_wrap, facet_col_spacing, facet_row_spacing, color_continuous_scale, color_continuous_midpoint, range_color, title, template, width, height, aspect, contrast_rescaling, binary_string, binary_backend, binary_compression_level, binary_format, text_auto)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0mlayout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"xaxis\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mautorange\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"reversed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0;34m\"px.imshow only accepts 2D single-channel, RGB or RGBA images. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;34m\"An image of shape %s was provided. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: px.imshow only accepts 2D single-channel, RGB or RGBA images. An image of shape (3, 65, 65, 1) was provided. Alternatively, 3- or 4-D single or multichannel datasets can be visualized using the `facet_col` or/and `animation_frame` arguments."
     ]
    }
   ],
   "source": [
    "from PIL import Image \n",
    "import plotly.express as px\n",
    "\n",
    "for example in ds_train.take(1):\n",
    "  im, lb = example\n",
    "#   cast_image = tf.cast(im.swapaxes(0, 3), 'float32')\n",
    "  print(im) # array of single band values\n",
    "#   px.imshow(im) # needs 3D RGB to plot\n",
    "#   print(\"Windspeed is\", str(lb.numpy())) \n",
    "#   display(Image.fromarray(tf.squeeze(im).numpy())) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scratch code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Attempt to convert a value (<TFRecordDatasetV2 shapes: (), types: tf.string>) with an unsupported type (<class 'tensorflow.python.data.ops.readers.TFRecordDatasetV2'>) to a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/q5/2d8pwqf14mv_nz14d23_2b040000gn/T/ipykernel_1666/1769582340.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mread_one_whole_rec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/q5/2d8pwqf14mv_nz14d23_2b040000gn/T/ipykernel_1666/431180165.py\u001b[0m in \u001b[0;36mread_one_whole_rec\u001b[0;34m(data, band)\u001b[0m\n\u001b[1;32m      6\u001b[0m             }\n\u001b[1;32m      7\u001b[0m         )\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfrecord_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mband\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m#expecting this original image size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.8/site-packages/tensorflow/python/ops/parsing_ops.py\u001b[0m in \u001b[0;36mparse_example_v2\u001b[0;34m(serialized, features, example_names, name)\u001b[0m\n\u001b[1;32m    312\u001b[0m   ])\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parse_example_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0m_construct_tensors_for_composite_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.8/site-packages/tensorflow/python/ops/parsing_ops.py\u001b[0m in \u001b[0;36m_parse_example_raw\u001b[0;34m(serialized, names, params, name)\u001b[0m\n\u001b[1;32m    344\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ParseExample\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mserialized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m     \u001b[0mserialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"serialized\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mragged_keys\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mserialized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m       raise ValueError(\"serialized must have statically-known rank to \"\n",
      "\u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1566\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    337\u001b[0m                                          as_ref=False):\n\u001b[1;32m    338\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0msymbolic\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m   \"\"\"\n\u001b[0;32m--> 264\u001b[0;31m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0m\u001b[1;32m    265\u001b[0m                         allow_broadcast=True)\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m   \u001b[0;34m\"\"\"Implementation of eager constant.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Attempt to convert a value (<TFRecordDatasetV2 shapes: (), types: tf.string>) with an unsupported type (<class 'tensorflow.python.data.ops.readers.TFRecordDatasetV2'>) to a Tensor."
     ]
    }
   ],
   "source": [
    "# does not work\n",
    "read_one_whole_rec(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[196, 195, 202],\n",
       "         [196, 201, 202],\n",
       "         [179, 183, 196],\n",
       "         ...,\n",
       "         [133, 154, 176],\n",
       "         [133, 154, 183],\n",
       "         [142, 160, 183]],\n",
       " \n",
       "        [[192, 195, 202],\n",
       "         [196, 201, 202],\n",
       "         [192, 195, 202],\n",
       "         ...,\n",
       "         [146, 160, 183],\n",
       "         [142, 160, 183],\n",
       "         [154, 166, 189]],\n",
       " \n",
       "        [[188, 189, 202],\n",
       "         [192, 195, 202],\n",
       "         [196, 201, 202],\n",
       "         ...,\n",
       "         [150, 166, 183],\n",
       "         [146, 160, 183],\n",
       "         [154, 166, 189]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[158, 160, 176],\n",
       "         [171, 166, 183],\n",
       "         [183, 171, 183],\n",
       "         ...,\n",
       "         [163, 177, 189],\n",
       "         [150, 177, 189],\n",
       "         [163, 177, 196]],\n",
       " \n",
       "        [[154, 154, 176],\n",
       "         [167, 160, 183],\n",
       "         [179, 171, 183],\n",
       "         ...,\n",
       "         [154, 171, 189],\n",
       "         [158, 171, 189],\n",
       "         [150, 171, 189]],\n",
       " \n",
       "        [[150, 154, 176],\n",
       "         [154, 160, 176],\n",
       "         [167, 166, 183],\n",
       "         ...,\n",
       "         [154, 166, 189],\n",
       "         [158, 166, 189],\n",
       "         [158, 166, 189]]], dtype=uint8),\n",
       " 0)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_img(file=\"gs://wagon-data-batch913-drought_detection/data/train/part-r-00000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: ((65, 65, 1), ()), types: (tf.uint8, tf.float32)>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('shims')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "65eef2ef16cca1c6c0509da0af7e988c9ac37c68d780cbc3b02fa330c2160ac2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
